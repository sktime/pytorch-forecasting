============================= test session starts ==============================
platform darwin -- Python 3.14.3, pytest-9.0.2, pluggy-1.6.0 -- /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/bin/python
cachedir: .cache
rootdir: /Users/admin/Desktop/open-source/pytorch-forecasting
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 13 items / 11 deselected / 2 selected
run-last-failure: rerun previous 2 failures

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] FAILED [ 50%]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3] FAILED [100%]

=================================== FAILURES ===================================
_ test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-31/test_decoder_mlp_supported_los0')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.NegativeBinomialDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        def int_target_collate(batch):
            # Handle both lists and tuples, filter out None
            filtered_batch = [item for item in batch if item is not None]
            if not filtered_batch:
                return {}, ()
    
            from torch.utils.data._utils.collate import default_collate
    
            # Collate x
            x_batch = [item[0] for item in filtered_batch]
            x_collated = default_collate(x_batch)
    
            # Collate y (target, weight)
            y_batch = [item[1] for item in filtered_batch]
            target_batch = [item[0] for item in y_batch]
            target_collated = default_collate(target_batch)
    
            weight_batch = [item[1] for item in y_batch]
            if any(w is None for w in weight_batch):
                weight_collated = None
            else:
                weight_collated = default_collate(weight_batch)
    
            # target should be long if we reach here (only used for NegativeBinomial)
            return x_collated, (target_collated.long(), weight_collated)
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:257: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1121: in _run_stage
    self._run_sanity_check()
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1150: in _run_sanity_check
    val_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/utilities.py:179: in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:146: in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:441: in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:329: in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:412: in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:741: in validation_step
    log, out = self.step(x, y, batch_idx)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DecoderMLP(
  	"activation_class":                  ReLU
  	"categorical_groups":                {}
  	"dataset_parame...Norm((8,), eps=1e-05, elementwise_affine=True)
      (16): Linear(in_features=8, out_features=2, bias=True)
    )
  )
)
x = {'decoder_length': tensor([3, 3]), 'encoder_length': tensor([4, 4]), 'encoder_target': tensor([[106.2720, 133.8120,  70.3080, 110.5920],
        [133.8120,  70.3080, 110.5920, 122.0400]]), 'encoder_time_idx_start': tensor([34, 35]), ...}
y = (tensor([[122, 115, 152],
        [115, 152, 128]]), None), batch_idx = 0
kwargs = {}

    def step(
        self,
        x: dict[str, torch.Tensor],
        y: tuple[torch.Tensor, torch.Tensor],
        batch_idx: int,
        **kwargs,
    ) -> tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:
        """
        Run for each train/val step.
    
        Args:
            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader
            y (Tuple[torch.Tensor, torch.Tensor]): y as passed to the loss function by the dataloader
            batch_idx (int): batch number
            **kwargs: additional arguments to pass to the network apart from ``x``
    
        Returns:
            Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]: tuple where the first
                entry is a dictionary to which additional logging results can be added for consumption in the
                ``on_epoch_end`` hook and the second entry is the model's output.
        """  # noqa: E501
        # pack y sequence if different encoder lengths exist
>       if (x["decoder_lengths"] < x["decoder_lengths"].max()).any():
            ^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'decoder_lengths'

pytorch_forecasting/models/base/_base_model.py:857: KeyError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                             | Params | Mode  | FLOPs
--------------------------------------------------------------------------------------
0 | loss             | NegativeBinomialDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                       | 0      | train | 0    
2 | input_embeddings | MultiEmbedding                   | 0      | train | 0    
3 | mlp              | FullyConnectedModule             | 314    | train | 0    
--------------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
_ test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-31/test_decoder_mlp_supported_los1')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.LogNormalDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='standard',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation='log',
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='standard',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation='log',
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        def int_target_collate(batch):
            # Handle both lists and tuples, filter out None
            filtered_batch = [item for item in batch if item is not None]
            if not filtered_batch:
                return {}, ()
    
            from torch.utils.data._utils.collate import default_collate
    
            # Collate x
            x_batch = [item[0] for item in filtered_batch]
            x_collated = default_collate(x_batch)
    
            # Collate y (target, weight)
            y_batch = [item[1] for item in filtered_batch]
            target_batch = [item[0] for item in y_batch]
            target_collated = default_collate(target_batch)
    
            weight_batch = [item[1] for item in y_batch]
            if any(w is None for w in weight_batch):
                weight_collated = None
            else:
                weight_collated = default_collate(weight_batch)
    
            # target should be long if we reach here (only used for NegativeBinomial)
            return x_collated, (target_collated.long(), weight_collated)
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:257: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1123: in _run_stage
    self.fit_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/fit_loop.py:217: in run
    self.advance()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/fit_loop.py:465: in advance
    self.epoch_loop.run(self._data_fetcher)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/training_epoch_loop.py:153: in run
    self.advance(data_fetcher)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/training_epoch_loop.py:352: in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/optimization/automatic.py:192: in run
    self._optimizer_step(batch_idx, closure)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/optimization/automatic.py:270: in _optimizer_step
    call._call_lightning_module_hook(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:177: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/core/module.py:1368: in optimizer_step
    optimizer.step(closure=optimizer_closure)
.venv/lib/python3.14/site-packages/lightning/pytorch/core/optimizer.py:154: in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:239: in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/plugins/precision/precision.py:123: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/optim/optimizer.py:526: in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/optim/optimizer.py:81: in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/optim/adam.py:227: in step
    loss = closure()
           ^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/plugins/precision/precision.py:109: in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/optimization/automatic.py:146: in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/utils/_contextlib.py:124: in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/optimization/automatic.py:140: in closure
    self._backward_fn(step_output.closure_loss)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/optimization/automatic.py:241: in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:329: in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:213: in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
.venv/lib/python3.14/site-packages/lightning/pytorch/plugins/precision/precision.py:73: in backward
    model.backward(tensor, *args, **kwargs)
.venv/lib/python3.14/site-packages/lightning/pytorch/core/module.py:1137: in backward
    loss.backward(*args, **kwargs)
.venv/lib/python3.14/site-packages/torch/_tensor.py:630: in backward
    torch.autograd.backward(
.venv/lib/python3.14/site-packages/torch/autograd/__init__.py:364: in backward
    _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(1.6667e+08),), args = ((None,), False, False, ())
kwargs = {'accumulate_grad': True, 'allow_unreachable': True}
attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

.venv/lib/python3.14/site-packages/torch/autograd/graph.py:865: RuntimeError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 49.84it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                      | Params | Mode  | FLOPs
-------------------------------------------------------------------------------
0 | loss             | LogNormalDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                | 0      | train | 0    
2 | input_embeddings | MultiEmbedding            | 0      | train | 0    
3 | mlp              | FullyConnectedModule      | 314    | train | 0    
-------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
--------------------------- Captured stdout teardown ---------------------------
                                                                   
=============================== warnings summary ===============================
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1006: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.
  See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.
    object_columns = data.head(1).select_dtypes(object).columns

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1885: Pandas4Warning: The copy keyword is deprecated and will be removed in a future version. Copy-on-Write is active in pandas since 3.0 which utilizes a lazy copy mechanism that defers copies until necessary. Use .copy() to make an eager copy if necessary.
    df_index = df_index[minimal_columns].astype("int32", copy=False)

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================ 2 failed, 11 deselected, 11 warnings in 0.63s =================
