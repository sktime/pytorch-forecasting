
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>BaseModel &#8212; pytorch-forecasting  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=9542a950" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://buttons.github.io/buttons.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/pytorch_forecasting.models.base._base_model.BaseModel';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BaseModelWithCovariates" href="pytorch_forecasting.models.base._base_model.BaseModelWithCovariates.html" />
    <link rel="prev" title="AutoRegressiveBaseModelWithCovariates" href="pytorch_forecasting.models.base._base_model.AutoRegressiveBaseModelWithCovariates.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.svg" class="logo__image only-light" alt="pytorch-forecasting  documentation - Home"/>
    <img src="../_static/logo.svg" class="logo__image only-dark pst-js-only" alt="pytorch-forecasting  documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting-started.html">
    Getting started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../data.html">
    Data
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../models.html">
    Models
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../metrics.html">
    Metrics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installation.html">
    Installation
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../CHANGELOG.html">
    Release Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://github.com/sktime/pytorch-forecasting">
    GitHub
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/sktime/pytorch-forecasting" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting-started.html">
    Getting started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../data.html">
    Data
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../models.html">
    Models
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../metrics.html">
    Metrics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installation.html">
    Installation
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../CHANGELOG.html">
    Release Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://github.com/sktime/pytorch-forecasting">
    GitHub
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/sktime/pytorch-forecasting" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_forecasting.data.html">data</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data._tslib_data_module.html">_tslib_data_module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data._tslib_data_module.TslibDataModule.html">TslibDataModule</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data._tslib_data_module._TslibDataset.html">_TslibDataset</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data.data_module.html">data_module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.data_module.EncoderDecoderTimeSeriesDataModule.html">EncoderDecoderTimeSeriesDataModule</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data.encoders.html">encoders</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._clipped_log.html">_clipped_log</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._clipped_logit.html">_clipped_logit</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._identity.html">_identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._minus_one.html">_minus_one</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._plus_one.html">_plus_one</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders._square.html">_square</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.softplus_inv.html">softplus_inv</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.EncoderNormalizer.html">EncoderNormalizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.Expm1Transform.html">Expm1Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.GroupNormalizer.html">GroupNormalizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.MinusOneTransform.html">MinusOneTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.MultiNormalizer.html">MultiNormalizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.NaNLabelEncoder.html">NaNLabelEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.ReLuTransform.html">ReLuTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.SoftplusTransform.html">SoftplusTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.TorchNormalizer.html">TorchNormalizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.encoders.TransformMixIn.html">TransformMixIn</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data.examples.html">examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.examples._get_data_by_filename.html">_get_data_by_filename</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.examples.generate_ar_data.html">generate_ar_data</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.examples.get_stallion_data.html">get_stallion_data</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.examples.load_toydata.html">load_toydata</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data.samplers.html">samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.samplers.GroupedSampler.html">GroupedSampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.data.samplers.TimeSynchronizedBatchSampler.html">TimeSynchronizedBatchSampler</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch_forecasting.data.tests.html">tests</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.data.timeseries.html">timeseries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.html">_timeseries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries._find_end_indices.html">_find_end_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.check_for_nonfinite.html">check_for_nonfinite</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html">TimeSeriesDataSet</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries_v2.html">_timeseries_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries_v2.TimeSeries.html">TimeSeries</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="pytorch_forecasting.models.html">models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="pytorch_forecasting.models.base.html">base</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active has-children"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.html">_base_model</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model._concatenate_output.html">_concatenate_output</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model._torch_cat_na.html">_torch_cat_na</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.AutoRegressiveBaseModel.html">AutoRegressiveBaseModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.AutoRegressiveBaseModelWithCovariates.html">AutoRegressiveBaseModelWithCovariates</a></li>
<li class="toctree-l4 current active"><a class="current reference internal" href="#">BaseModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.BaseModelWithCovariates.html">BaseModelWithCovariates</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.PredictCallback.html">PredictCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.PredictTuple.html">PredictTuple</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.Prediction.html">Prediction</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.base._base_model_v2.html">_base_model_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_model_v2.BaseModel.html">BaseModel</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.base._base_object.html">_base_object</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_object._BasePtForecaster.html">_BasePtForecaster</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_object._BasePtForecasterV2.html">_BasePtForecasterV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._base_object._BasePtForecaster_Common.html">_BasePtForecaster_Common</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.base._tslib_base_model_v2.html">_tslib_base_model_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.base._tslib_base_model_v2.TslibBaseModel.html">TslibBaseModel</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch_forecasting.models.base_model.html">base_model</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.baseline.html">baseline</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.models.baseline.Baseline.html">Baseline</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.deepar.html">deepar</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.deepar._deepar.html">_deepar</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.deepar._deepar.DeepAR.html">DeepAR</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.deepar._deepar_pkg.html">_deepar_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.deepar._deepar_pkg.DeepAR_pkg.html">DeepAR_pkg</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.dlinear.html">dlinear</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.dlinear._dlinear_pkg_v2.html">_dlinear_pkg_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.dlinear._dlinear_pkg_v2.DLinear_pkg_v2.html">DLinear_pkg_v2</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.dlinear._dlinear_v2.html">_dlinear_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.dlinear._dlinear_v2.DLinear.html">DLinear</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.mlp.html">mlp</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.mlp._decodermlp.html">_decodermlp</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.mlp._decodermlp.DecoderMLP.html">DecoderMLP</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.mlp._decodermlp_pkg.html">_decodermlp_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.mlp._decodermlp_pkg.DecoderMLP_pkg.html">DecoderMLP_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.mlp.submodules.html">submodules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.mlp.submodules.FullyConnectedModule.html">FullyConnectedModule</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.nbeats.html">nbeats</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nbeats._nbeats.html">_nbeats</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats._nbeats.NBeats.html">NBeats</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nbeats._nbeats_pkg.html">_nbeats_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats._nbeats_pkg.NBeats_pkg.html">NBeats_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.html">sub_modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.linear.html">linear</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.linspace.html">linspace</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.NBEATSBlock.html">NBEATSBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.NBEATSGenericBlock.html">NBEATSGenericBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.NBEATSSeasonalBlock.html">NBEATSSeasonalBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nbeats.sub_modules.NBEATSTrendBlock.html">NBEATSTrendBlock</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.nhits.html">nhits</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nhits._nhits.html">_nhits</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits._nhits.NHiTS.html">NHiTS</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nhits._nhits_pkg.html">_nhits_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits._nhits_pkg.NHiTS_pkg.html">NHiTS_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.html">sub_modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.init_weights.html">init_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.IdentityBasis.html">IdentityBasis</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.MLP.html">MLP</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.NHiTS.html">NHiTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.NHiTSBlock.html">NHiTSBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nhits.sub_modules.StaticFeaturesEncoder.html">StaticFeaturesEncoder</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.nn.html">nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nn.embeddings.html">embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.embeddings.MultiEmbedding.html">MultiEmbedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.embeddings.TimeDistributedEmbeddingBag.html">TimeDistributedEmbeddingBag</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.nn.rnn.html">rnn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.rnn.get_rnn.html">get_rnn</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.rnn.GRU.html">GRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.rnn.LSTM.html">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.nn.rnn.RNN.html">RNN</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.rnn.html">rnn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.rnn._rnn.html">_rnn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.rnn._rnn.RecurrentNetwork.html">RecurrentNetwork</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.rnn._rnn_pkg.html">_rnn_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.rnn._rnn_pkg.RecurrentNetwork_pkg.html">RecurrentNetwork_pkg</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.html">temporal_fusion_transformer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft.html">_tft</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft.TemporalFusionTransformer.html">TemporalFusionTransformer</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_pkg.html">_tft_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_pkg.TemporalFusionTransformer_pkg.html">TemporalFusionTransformer_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_pkg_v2.html">_tft_pkg_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_pkg_v2.TFT_pkg_v2.html">TFT_pkg_v2</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_v2.html">_tft_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer._tft_v2.TFT.html">TFT</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.html">sub_modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.AddNorm.html">AddNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.GateAddNorm.html">GateAddNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.GatedLinearUnit.html">GatedLinearUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.GatedResidualNetwork.html">GatedResidualNetwork</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.InterpretableMultiHeadAttention.html">InterpretableMultiHeadAttention</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.PositionalEncoder.html">PositionalEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.ResampleNorm.html">ResampleNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.ScaledDotProductAttention.html">ScaledDotProductAttention</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.TimeDistributed.html">TimeDistributed</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.TimeDistributedInterpolation.html">TimeDistributedInterpolation</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.sub_modules.VariableSelectionNetwork.html">VariableSelectionNetwork</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.tuning.html">tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.tuning._lazywhere.html">_lazywhere</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.temporal_fusion_transformer.tuning.optimize_hyperparameters.html">optimize_hyperparameters</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.tide.html">tide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.tide._tide.html">_tide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide._tide.TiDEModel.html">TiDEModel</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.tide._tide_dsipts.html">_tide_dsipts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide._tide_dsipts._tide_v2.html">_tide_v2</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide._tide_dsipts._tide_v2_pkg.html">_tide_v2_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.tide._tide_pkg.html">_tide_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide._tide_pkg.TiDEModel_pkg.html">TiDEModel_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.tide.sub_modules.html">sub_modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide.sub_modules._ResidualBlock.html">_ResidualBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.tide.sub_modules._TideModule.html">_TideModule</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer.html">timexer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer.html">_timexer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer.TimeXer.html">TimeXer</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_pkg.html">_timexer_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_pkg.TimeXer_pkg.html">TimeXer_pkg</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_pkg_v2.html">_timexer_pkg_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_pkg_v2.TimeXer_pkg_v2.html">TimeXer_pkg_v2</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_v2.html">_timexer_v2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer._timexer_v2.TimeXer.html">TimeXer</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.html">sub_modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.AttentionLayer.html">AttentionLayer</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.DataEmbedding_inverted.html">DataEmbedding_inverted</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.EnEmbedding.html">EnEmbedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.Encoder.html">Encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.EncoderLayer.html">EncoderLayer</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.FlattenHead.html">FlattenHead</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.FullAttention.html">FullAttention</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.PositionalEmbedding.html">PositionalEmbedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.timexer.sub_modules.TriangularCausalMask.html">TriangularCausalMask</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.models.xlstm.html">xlstm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.xlstm._xlstm.html">_xlstm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.xlstm._xlstm.xLSTMTime.html">xLSTMTime</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.models.xlstm._xlstm_pkg.html">_xlstm_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.models.xlstm._xlstm_pkg.xLSTMTime_pkg.html">xLSTMTime_pkg</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.html">metrics</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch_forecasting.metrics._distributions_pkg.html">_distributions_pkg</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics._mqf2_utils.html">_mqf2_utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics._mqf2_utils.DeepConvexNet.html">DeepConvexNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics._mqf2_utils.MQF2Distribution.html">MQF2Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics._mqf2_utils.SequentialNet.html">SequentialNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics._mqf2_utils.TransformedMQF2Distribution.html">TransformedMQF2Distribution</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch_forecasting.metrics._point_pkg.html">_point_pkg</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics._quantile_pkg.html">_quantile_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.metrics._quantile_pkg._quantile_loss_pkg.html">_quantile_loss_pkg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics._quantile_pkg._quantile_loss_pkg.QuantileLoss_pkg.html">QuantileLoss_pkg</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics.html">base_metrics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.html">_base_metrics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.convert_torchmetric_to_pytorch_forecasting_metric.html">convert_torchmetric_to_pytorch_forecasting_metric</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.AggregationMetric.html">AggregationMetric</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.CompositeMetric.html">CompositeMetric</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.DistributionLoss.html">DistributionLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html">Metric</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric.html">MultiHorizonMetric</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiLoss.html">MultiLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultivariateDistributionLoss.html">MultivariateDistributionLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.TorchMetricWrapper.html">TorchMetricWrapper</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_object.html">_base_object</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_object._BasePtMetric.html">_BasePtMetric</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.html">distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.BetaDistributionLoss.html">BetaDistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.ImplicitQuantileNetwork.html">ImplicitQuantileNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.ImplicitQuantileNetworkDistributionLoss.html">ImplicitQuantileNetworkDistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.LogNormalDistributionLoss.html">LogNormalDistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.MQF2DistributionLoss.html">MQF2DistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.MultivariateNormalDistributionLoss.html">MultivariateNormalDistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.NegativeBinomialDistributionLoss.html">NegativeBinomialDistributionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.distributions.NormalDistributionLoss.html">NormalDistributionLoss</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.point.html">point</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.CrossEntropy.html">CrossEntropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.MAE.html">MAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.MAPE.html">MAPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.MASE.html">MASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.PoissonLoss.html">PoissonLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.RMSE.html">RMSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.SMAPE.html">SMAPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.point.TweedieLoss.html">TweedieLoss</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.metrics.quantile.html">quantile</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.metrics.quantile.QuantileLoss.html">QuantileLoss</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_forecasting.utils.html">utils</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.utils._classproperty.html">_classproperty</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._classproperty.classproperty.html">classproperty</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.utils._coerce.html">_coerce</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._coerce._coerce_to_dict.html">_coerce_to_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._coerce._coerce_to_list.html">_coerce_to_list</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.utils._dependencies.html">_dependencies</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.utils._dependencies._dependencies.html">_dependencies</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._dependencies._dependencies._check_matplotlib.html">_check_matplotlib</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._dependencies._dependencies._get_installed_packages.html">_get_installed_packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._dependencies._dependencies._get_installed_packages_private.html">_get_installed_packages_private</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._dependencies._safe_import.html">_safe_import</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.utils._dependencies.tests.html">tests</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._dependencies.tests.test_safe_import.html">test_safe_import</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.utils._maint.html">_maint</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pytorch_forecasting.utils._maint._show_versions.html">_show_versions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._maint._show_versions._get_deps_info.html">_get_deps_info</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._maint._show_versions._get_sys_info.html">_get_sys_info</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytorch_forecasting.utils._maint._show_versions.show_versions.html">show_versions</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch_forecasting.utils._utils.html">_utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.apply_to_list.html">apply_to_list</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.autocorrelation.html">autocorrelation</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.concat_sequences.html">concat_sequences</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.create_mask.html">create_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.detach.html">detach</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.get_embedding_size.html">get_embedding_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.groupby_apply.html">groupby_apply</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.integer_histogram.html">integer_histogram</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.masked_op.html">masked_op</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.move_to_device.html">move_to_device</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.next_fast_len.html">next_fast_len</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.padded_stack.html">padded_stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.profile.html">profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.repr_class.html">repr_class</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.to_list.html">to_list</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.unpack_sequence.html">unpack_sequence</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.unsqueeze_like.html">unsqueeze_like</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.InitialParameterRepresenterMixIn.html">InitialParameterRepresenterMixIn</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.OutputMixIn.html">OutputMixIn</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_forecasting.utils._utils.TupleOutputMixIn.html">TupleOutputMixIn</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../getting-started.html" class="nav-link">Getting started</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">BaseModel</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="basemodel">
<h1>BaseModel<a class="headerlink" href="#basemodel" title="Link to this heading">#</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_forecasting.models.base._base_model.</span></span><span class="sig-name descname"><span class="pre">BaseModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_parameters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_val_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_gradient_flow</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.Metric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric"><span class="pre">Metric</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SMAPE()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logging_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleList</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ModuleList()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_on_plateau_patience</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_on_plateau_reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_on_plateau_min_lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">monotone_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transformer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytorch_forecasting.utils._utils.InitialParameterRepresenterMixIn.html#pytorch_forecasting.utils._utils.InitialParameterRepresenterMixIn" title="pytorch_forecasting.utils._utils.InitialParameterRepresenterMixIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">InitialParameterRepresenterMixIn</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>, <a class="reference internal" href="pytorch_forecasting.utils._utils.TupleOutputMixIn.html#pytorch_forecasting.utils._utils.TupleOutputMixIn" title="pytorch_forecasting.utils._utils.TupleOutputMixIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">TupleOutputMixIn</span></code></a></p>
<p>BaseModel from which new timeseries models should inherit from.
The <code class="docutils literal notranslate"><span class="pre">hparams</span></code> of the created object will default to the parameters indicated in <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code>.</p>
<p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.forward" title="pytorch_forecasting.models.base._base_model.BaseModel.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method should return a named tuple with at least the entry <code class="docutils literal notranslate"><span class="pre">prediction</span></code>
that contains the network’s output. See the function’s documentation for more details.</p>
<p>The idea of the base model is that common methods do not have to be re-implemented for every new architecture.
The class is a [LightningModule](<a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html">https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html</a>)
and follows its conventions. However, there are important additions:</p>
<blockquote>
<div><ul class="simple">
<li><p>You need to specify a <code class="docutils literal notranslate"><span class="pre">loss</span></code> attribute that stores the function to calculate the
<code class="xref py py-class docutils literal notranslate"><span class="pre">MultiHorizonLoss</span></code> for backpropagation.</p></li>
<li><p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.from_dataset" title="pytorch_forecasting.models.base._base_model.BaseModel.from_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_dataset()</span></code></a> method can be used to initialize a network using the specifications
of a dataset. Often, parameters such as the number of features can be easily deduced from the dataset.
Further, the method will also store how to rescale normalized predictions into the unnormalized prediction
space. Override it to pass additional arguments to the __init__ method of your network that depend on your
dataset.</p></li>
<li><p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output" title="pytorch_forecasting.models.base._base_model.BaseModel.transform_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform_output()</span></code></a> method rescales the network output using the target normalizer
from thedataset.</p></li>
<li><p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.step" title="pytorch_forecasting.models.base._base_model.BaseModel.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a> method takes care of calculating the loss, logging additional metrics defined
in the <code class="docutils literal notranslate"><span class="pre">logging_metrics</span></code> attribute and plots of sample predictions. You can override this method to add
custom interpretations or pass extra arguments to the networks forward method.</p></li>
<li><p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end" title="pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_epoch_end()</span></code></a> method can be used to calculate summaries of each epoch such as
statistics on the encoder length, etc and needs to return the outputs.</p></li>
<li><p>The <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict" title="pytorch_forecasting.models.base._base_model.BaseModel.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a> method makes predictions using a dataloader or dataset. Override it if you
need to pass additional arguments to <code class="docutils literal notranslate"><span class="pre">forward</span></code> by default.</p></li>
</ul>
</div></blockquote>
<p>To implement your own architecture, it is best to
go throught the <span class="xref std std-ref">Using custom data and implementing custom models</span> and
to look at existing ones to understand what might be a good approach.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">my_first_parameter</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">SMAPE</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">normalized_prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_output</span><span class="p">(</span><span class="n">prediction</span><span class="o">=</span><span class="n">normalized_prediction</span><span class="p">,</span> <span class="n">target_scale</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;target_scale&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_network_output</span><span class="p">(</span><span class="n">prediction</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
<p>BaseModel for timeseries forecasting from which to inherit from</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_interval</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – Batches after which predictions are logged. If &lt; 1.0, will log
multiple entries per batch. Defaults to -1.</p></li>
<li><p><strong>log_val_interval</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – batches after which predictions for validation are
logged. Defaults to None/log_interval.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate. Defaults to 1e-3.</p></li>
<li><p><strong>log_gradient_flow</strong> (<em>bool</em>) – If to log gradient flow, this takes time and should be only done to diagnose
training failures. Defaults to False.</p></li>
<li><p><strong>loss</strong> (<a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.Metric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric"><em>Metric</em></a><em>, </em><em>optional</em>) – metric to optimize, can also be list of metrics. Defaults to SMAPE().</p></li>
<li><p><strong>logging_metrics</strong> (<em>nn.ModuleList</em><em>[</em><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric"><em>MultiHorizonMetric</em></a><em>]</em>) – list of metrics that are logged during training.
Defaults to [].</p></li>
<li><p><strong>reduce_on_plateau_patience</strong> (<em>int</em>) – patience after which learning rate is reduced by a factor of 10. Defaults
to 1000</p></li>
<li><p><strong>reduce_on_plateau_reduction</strong> (<em>float</em>) – reduction in learning rate when encountering plateau. Defaults to 2.0.</p></li>
<li><p><strong>reduce_on_plateau_min_lr</strong> (<em>float</em>) – minimum learning rate for reduce on plateua learning rate scheduler.
Defaults to 1e-5</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em>) – weight decay. Defaults to 0.0.</p></li>
<li><p><strong>optimizer_params</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – additional parameters for the optimizer. Defaults to {}.</p></li>
<li><p><strong>monotone_constraints</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – dictionary of monotonicity constraints for continuous decoder
variables mapping
position (e.g. <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code> for first position) to constraint (<code class="docutils literal notranslate"><span class="pre">-1</span></code> for negative and <code class="docutils literal notranslate"><span class="pre">+1</span></code> for positive,
larger numbers add more weight to the constraint vs. the loss but are usually not necessary).
This constraint significantly slows down training. Defaults to {}.</p></li>
<li><p><strong>output_transformer</strong> (<em>Callable</em>) – transformer that takes network output and transforms it to prediction space.
Defaults to None which is equivalent to <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">out:</span> <span class="pre">out[&quot;prediction&quot;]</span></code>.</p></li>
<li><p><strong>optimizer</strong> (<em>str</em>) – Optimizer, “ranger”, “sgd”, “adam”, “adamw” or class name of optimizer in <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>
or <code class="docutils literal notranslate"><span class="pre">pytorch_optimizer</span></code>.
Alternatively, a class or function can be passed which takes parameters as first argument and
a <cite>lr</cite> argument (optionally also <cite>weight_decay</cite>). Defaults to “adam”.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Methods</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.configure_optimizers" title="pytorch_forecasting.models.base._base_model.BaseModel.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>Configure optimizers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.create_log" title="pytorch_forecasting.models.base._base_model.BaseModel.create_log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_log</span></code></a>(x, y, out, batch_idx[, ...])</p></td>
<td><p>Create the log used in the training and validation step.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.deduce_default_output_parameters" title="pytorch_forecasting.models.base._base_model.BaseModel.deduce_default_output_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deduce_default_output_parameters</span></code></a>(dataset, kwargs)</p></td>
<td><p>Deduce default parameters for output for <cite>from_dataset()</cite> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.forward" title="pytorch_forecasting.models.base._base_model.BaseModel.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(x)</p></td>
<td><p>Network forward pass.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.from_dataset" title="pytorch_forecasting.models.base._base_model.BaseModel.from_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_dataset</span></code></a>(dataset, **kwargs)</p></td>
<td><p>Create model from dataset, i.e. save dataset parameters in model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.log" title="pytorch_forecasting.models.base._base_model.BaseModel.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(*args, **kwargs)</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">lightning.pytorch.core.lightning.LightningModule.log()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_gradient_flow" title="pytorch_forecasting.models.base._base_model.BaseModel.log_gradient_flow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_gradient_flow</span></code></a>(named_parameters)</p></td>
<td><p>log distribution of gradients to identify exploding / vanishing gradients</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_metrics" title="pytorch_forecasting.models.base._base_model.BaseModel.log_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_metrics</span></code></a>(x, y, out[, prediction_kwargs])</p></td>
<td><p>Log metrics every training/validation step.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_prediction" title="pytorch_forecasting.models.base._base_model.BaseModel.log_prediction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_prediction</span></code></a>(x, out, batch_idx, **kwargs)</p></td>
<td><p>Log metrics every training/validation step.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_after_backward" title="pytorch_forecasting.models.base._base_model.BaseModel.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Log gradient flow for debugging.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end" title="pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Run at epoch end for training or validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_load_checkpoint" title="pytorch_forecasting.models.base._base_model.BaseModel.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint" title="pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_test_epoch_end" title="pytorch_forecasting.models.base._base_model.BaseModel.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_train_epoch_end" title="pytorch_forecasting.models.base._base_model.BaseModel.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_validation_epoch_end" title="pytorch_forecasting.models.base._base_model.BaseModel.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.plot_prediction" title="pytorch_forecasting.models.base._base_model.BaseModel.plot_prediction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_prediction</span></code></a>(x, out[, idx, ...])</p></td>
<td><p>Plot prediction of prediction vs actuals</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict" title="pytorch_forecasting.models.base._base_model.BaseModel.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(data[, mode, return_index, ...])</p></td>
<td><p>Run inference / prediction.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_dependency" title="pytorch_forecasting.models.base._base_model.BaseModel.predict_dependency"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dependency</span></code></a>(data, variable, values[, ...])</p></td>
<td><p>Predict partial dependency.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_step" title="pytorch_forecasting.models.base._base_model.BaseModel.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx)</p></td>
<td><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.size" title="pytorch_forecasting.models.base._base_model.BaseModel.size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">size</span></code></a>()</p></td>
<td><p>get number of parameters in model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.step" title="pytorch_forecasting.models.base._base_model.BaseModel.step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step</span></code></a>(x, y, batch_idx, **kwargs)</p></td>
<td><p>Run for each train/val step.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.test_step" title="pytorch_forecasting.models.base._base_model.BaseModel.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(batch, batch_idx)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_prediction" title="pytorch_forecasting.models.base._base_model.BaseModel.to_prediction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_prediction</span></code></a>(out[, use_metric])</p></td>
<td><p>Convert output to prediction using the loss metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_quantiles" title="pytorch_forecasting.models.base._base_model.BaseModel.to_quantiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_quantiles</span></code></a>(out[, use_metric])</p></td>
<td><p>Convert output to quantiles using the loss metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.training_step" title="pytorch_forecasting.models.base._base_model.BaseModel.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(batch, batch_idx)</p></td>
<td><p>Train on batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output" title="pytorch_forecasting.models.base._base_model.BaseModel.transform_output"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform_output</span></code></a>(prediction, target_scale[, ...])</p></td>
<td><p>Extract prediction from network output and rescale it to real space / de-normalize it.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.validation_step" title="pytorch_forecasting.models.base._base_model.BaseModel.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(batch, batch_idx)</p></td>
<td><p>Operates on a single batch of data from the validation set.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.configure_optimizers" title="Link to this definition">#</a></dt>
<dd><p>Configure optimizers.</p>
<p>Uses single Ranger optimizer. Depending if learning rate is a list or a single float, implement dynamic
learning rate scheduler or deterministic version</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>first entry is list of optimizers and second is list of schedulers</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple[List]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.create_log">
<span class="sig-name descname"><span class="pre">create_log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantiles_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.create_log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.create_log" title="Link to this definition">#</a></dt>
<dd><p>Create the log used in the training and validation step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – x as passed to the network by the dataloader</p></li>
<li><p><strong>y</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – y as passed to the loss function by the dataloader</p></li>
<li><p><strong>out</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – output of the network</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – batch number</p></li>
<li><p><strong>prediction_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>optional</em>) – arguments to pass to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to_prediction()</span></code>. Defaults to {}.</p></li>
<li><p><strong>quantiles_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>optional</em>) – <code class="xref py py-meth docutils literal notranslate"><span class="pre">to_quantiles()</span></code>. Defaults to {}.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>log dictionary to be returned by training and validation steps</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.deduce_default_output_parameters">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">deduce_default_output_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><span class="pre">TimeSeriesDataSet</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric"><span class="pre">MultiHorizonMetric</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.deduce_default_output_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.deduce_default_output_parameters" title="Link to this definition">#</a></dt>
<dd><p>Deduce default parameters for output for <cite>from_dataset()</cite> method.</p>
<p>Determines <code class="docutils literal notranslate"><span class="pre">output_size</span></code> and <code class="docutils literal notranslate"><span class="pre">loss</span></code> parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><em>TimeSeriesDataSet</em></a>) – timeseries dataset</p></li>
<li><p><strong>kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – current hyperparameters</p></li>
<li><p><strong>default_loss</strong> (<a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.MultiHorizonMetric"><em>MultiHorizonMetric</em></a><em>, </em><em>optional</em>) – default loss function.
Defaults to <code class="xref py py-class docutils literal notranslate"><span class="pre">MAE</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>dictionary with <code class="docutils literal notranslate"><span class="pre">output_size</span></code> and <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.forward" title="Link to this definition">#</a></dt>
<dd><p>Network forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>List</em><em>[</em><em>torch.Tensor</em><em>]</em><em>]</em><em>]</em>) – network input (x as returned by the dataloader).
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">to_dataloader()</span></code> method that
returns a tuple of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>. This function expects <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl>
<dt>network outputs / dictionary of tensors or list</dt><dd><p>of tensors. Create it using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to_network_output()</span></code> method.
The minimal required entries in the dictionary are (and shapes in brackets):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prediction</span></code> (batch_size x n_decoder_time_steps x n_outputs or list thereof with each
entry for a different target): re-scaled predictions that can be fed to metric. List of tensors
if multiple targets are predicted at the same time.</p></li>
</ul>
<p>Before passing outputting the predictions, you want to rescale them into real space.
By default, you can use the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transform_output()</span></code>
method to achieve this.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NamedTuple[Union[torch.Tensor, List[torch.Tensor]]]</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span>
    <span class="c1"># x is a batch generated based on the TimeSeriesDataset, here we just use the</span>
    <span class="c1"># continuous variables for the encoder</span>
    <span class="n">network_input</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;encoder_cont&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">network_input</span><span class="p">)</span>  <span class="c1">#</span>

    <span class="c1"># rescale predictions into target space</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_output</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target_scale</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;target_scale&quot;</span><span class="p">])</span>

    <span class="c1"># We need to return a dictionary that at least contains the prediction</span>
    <span class="c1"># The parameter can be directly forwarded from the input.</span>
    <span class="c1"># The conversion to a named tuple can be directly achieved with the `to_network_output` function.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_network_output</span><span class="p">(</span><span class="n">prediction</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.from_dataset">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><span class="pre">TimeSeriesDataSet</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LightningModule</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.from_dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.from_dataset" title="Link to this definition">#</a></dt>
<dd><p>Create model from dataset, i.e. save dataset parameters in model</p>
<p>This function should be called as <code class="docutils literal notranslate"><span class="pre">super().from_dataset()</span></code> in a derived models that implement it</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dataset</strong> (<a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><em>TimeSeriesDataSet</em></a>) – timeseries dataset</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Model that can be trained</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel" title="pytorch_forecasting.models.base._base_model.BaseModel">BaseModel</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.log" title="Link to this definition">#</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">lightning.pytorch.core.lightning.LightningModule.log()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.log_gradient_flow">
<span class="sig-name descname"><span class="pre">log_gradient_flow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">named_parameters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.log_gradient_flow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_gradient_flow" title="Link to this definition">#</a></dt>
<dd><p>log distribution of gradients to identify exploding / vanishing gradients</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.log_metrics">
<span class="sig-name descname"><span class="pre">log_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.log_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_metrics" title="Link to this definition">#</a></dt>
<dd><p>Log metrics every training/validation step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – x as passed to the network by the dataloader</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – y as passed to the loss function by the dataloader</p></li>
<li><p><strong>out</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – output of the network</p></li>
<li><p><strong>prediction_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – parameters for <code class="docutils literal notranslate"><span class="pre">to_prediction()</span></code> of the loss metric.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.log_prediction">
<span class="sig-name descname"><span class="pre">log_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.log_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_prediction" title="Link to this definition">#</a></dt>
<dd><p>Log metrics every training/validation step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – x as passed to the network by the dataloader</p></li>
<li><p><strong>out</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – output of the network</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – current batch index</p></li>
<li><p><strong>**kwargs</strong> – paramters to pass to <code class="docutils literal notranslate"><span class="pre">plot_prediction</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_after_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_after_backward" title="Link to this definition">#</a></dt>
<dd><p>Log gradient flow for debugging.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Run at epoch end for training or validation. Can be overriden in models.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_load_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_load_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called by Lightning to restore your model. If you saved something with <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint" title="pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is
your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong> – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to
save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong> – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_test_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_test_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_train_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_train_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> and access them in this hook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># do something with all training_step outputs, for example:</span>
        <span class="n">epoch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;training_epoch_mean&quot;</span><span class="p">,</span> <span class="n">epoch_mean</span><span class="p">)</span>
        <span class="c1"># free up the memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.on_validation_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_validation_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.plot_prediction">
<span class="sig-name descname"><span class="pre">plot_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_loss_to_title</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.Metric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric"><span class="pre">Metric</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_future_observed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantiles_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.plot_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.plot_prediction" title="Link to this definition">#</a></dt>
<dd><p>Plot prediction of prediction vs actuals</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – network input</p></li>
<li><p><strong>out</strong> – network output</p></li>
<li><p><strong>idx</strong> – index of prediction to plot</p></li>
<li><p><strong>add_loss_to_title</strong> – if to add loss to title or loss function to calculate. Can be either metrics,
bool indicating if to use loss metric or tensor which contains losses for all samples.
Calcualted losses are determined without weights. Default to False.</p></li>
<li><p><strong>show_future_observed</strong> – if to show actuals for future. Defaults to True.</p></li>
<li><p><strong>ax</strong> – matplotlib axes to plot on</p></li>
<li><p><strong>quantiles_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – parameters for <code class="docutils literal notranslate"><span class="pre">to_quantiles()</span></code> of the loss metric.</p></li>
<li><p><strong>prediction_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – parameters for <code class="docutils literal notranslate"><span class="pre">to_prediction()</span></code> of the loss metric.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>matplotlib figure</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataLoader</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><span class="pre">TimeSeriesDataSet</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_decoder_lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_dev_run</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'batch'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'epoch'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch_and_epoch'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'batch'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="pytorch_forecasting.models.base._base_model.Prediction.html#pytorch_forecasting.models.base._base_model.Prediction" title="pytorch_forecasting.models.base._base_model.Prediction"><span class="pre">Prediction</span></a></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict" title="Link to this definition">#</a></dt>
<dd><p>Run inference / prediction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataloader</strong> – dataloader, dataframe or dataset</p></li>
<li><p><strong>mode</strong> – one of “prediction”, “quantiles”, or “raw”, or tuple <code class="docutils literal notranslate"><span class="pre">(&quot;raw&quot;,</span> <span class="pre">output_name)</span></code> where output_name is
a name in the dictionary returned by <code class="docutils literal notranslate"><span class="pre">forward()</span></code></p></li>
<li><p><strong>return_index</strong> – if to return the prediction index (in the same order as the output, i.e. the row of the
dataframe corresponds to the first dimension of the output and the given time index is the time index
of the first prediction)</p></li>
<li><p><strong>return_decoder_lengths</strong> – if to return decoder_lengths (in the same order as the output</p></li>
<li><p><strong>batch_size</strong> – batch size for dataloader - only used if data is not a dataloader is passed</p></li>
<li><p><strong>num_workers</strong> – number of workers for dataloader - only used if data is not a dataloader is passed</p></li>
<li><p><strong>fast_dev_run</strong> – if to only return results of first batch</p></li>
<li><p><strong>return_x</strong> – if to return network inputs (in the same order as prediction output)</p></li>
<li><p><strong>return_y</strong> – if to return network targets (in the same order as prediction output)</p></li>
<li><p><strong>mode_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – keyword arguments for <code class="docutils literal notranslate"><span class="pre">to_prediction()</span></code> or <code class="docutils literal notranslate"><span class="pre">to_quantiles()</span></code>
for modes “prediction” and “quantiles”</p></li>
<li><p><strong>trainer_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>optional</em>) – keyword arguments for the trainer</p></li>
<li><p><strong>write_interval</strong> – interval to write predictions to disk</p></li>
<li><p><strong>output_dir</strong> – directory to write predictions to. Defaults to None. If set function will return empty list</p></li>
<li><p><strong>**kwargs</strong> – additional arguments to network’s forward method</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>if one of the <code class="docutils literal notranslate"><span class="pre">`return</span></code> arguments is present,</dt><dd><p>prediction tuple with fields <code class="docutils literal notranslate"><span class="pre">prediction</span></code>, <code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">decoder_lengths</span></code></p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="pytorch_forecasting.models.base._base_model.Prediction.html#pytorch_forecasting.models.base._base_model.Prediction" title="pytorch_forecasting.models.base._base_model.Prediction">Prediction</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.predict_dependency">
<span class="sig-name descname"><span class="pre">predict_dependency</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataLoader</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><span class="pre">TimeSeriesDataSet</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'dataframe'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'decoder'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.predict_dependency"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_dependency" title="Link to this definition">#</a></dt>
<dd><p>Predict partial dependency.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>Union</em><em>[</em><em>DataLoader</em><em>, </em><em>pd.DataFrame</em><em>, </em><a class="reference internal" href="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet" title="pytorch_forecasting.data.timeseries._timeseries.TimeSeriesDataSet"><em>TimeSeriesDataSet</em></a><em>]</em>) – data</p></li>
<li><p><strong>variable</strong> (<em>str</em>) – variable which to modify</p></li>
<li><p><strong>values</strong> (<em>Iterable</em>) – array of values to probe</p></li>
<li><p><strong>mode</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Output mode. Defaults to “dataframe”. Either</p>
<ul>
<li><p>”series”: values are average prediction and index are probed values</p></li>
<li><dl class="simple">
<dt>”dataframe”: columns are as obtained by the <cite>dataset.x_to_index()</cite> method,</dt><dd><p>prediction (which is the mean prediction over the time horizon),
normalized_prediction (which are predictions devided by the prediction for the first probed value)
the variable name for the probed values</p>
</dd>
</dl>
</li>
<li><p>”raw”: outputs a tensor of shape len(values) x prediction_shape</p></li>
</ul>
</p></li>
<li><p><strong>target</strong> – Defines which values are overwritten for making a prediction.
Same as in <code class="xref py py-meth docutils literal notranslate"><span class="pre">set_overwrite_values()</span></code>.
Defaults to “decoder”.</p></li>
<li><p><strong>show_progress_bar</strong> – if to show progress bar. Defaults to False.</p></li>
<li><p><strong>**kwargs</strong> – additional kwargs to <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict" title="pytorch_forecasting.models.base._base_model.BaseModel.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>output</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[np.ndarray, torch.Tensor, pd.Series, pd.DataFrame]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_step" title="Link to this definition">#</a></dt>
<dd><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>. By default, it calls
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>. Override to add any processing logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(strategy=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;tpu&quot;,</span> <span class="pre">devices=8)</span></code> as predictions won’t be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The output of your data iterable, normally a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><strong>batch_idx</strong> – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> – The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Predicted output (optional).</p>
</dd>
</dl>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.size">
<span class="sig-name descname"><span class="pre">size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.size" title="Link to this definition">#</a></dt>
<dd><p>get number of parameters in model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.step" title="Link to this definition">#</a></dt>
<dd><p>Run for each train/val step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – x as passed to the network by the dataloader</p></li>
<li><p><strong>y</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – y as passed to the loss function by the dataloader</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – batch number</p></li>
<li><p><strong>**kwargs</strong> – additional arguments to pass to the network apart from <code class="docutils literal notranslate"><span class="pre">x</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>tuple where the first</dt><dd><p>entry is a dictionary to which additional logging results can be added for consumption in the
<code class="docutils literal notranslate"><span class="pre">on_epoch_end</span></code> hook and the second entry is the model’s output.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.test_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the test set. In this step you’d normally generate examples or
calculate anything of interest such as accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The output of your data iterable, normally a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><strong>batch_idx</strong> – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> – The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.test_step" title="pytorch_forecasting.models.base._base_model.BaseModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;test_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;test_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.test_step" title="pytorch_forecasting.models.base._base_model.BaseModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.to_prediction">
<span class="sig-name descname"><span class="pre">to_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.to_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_prediction" title="Link to this definition">#</a></dt>
<dd><p>Convert output to prediction using the loss metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – output of network where “prediction” has been
transformed with <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output" title="pytorch_forecasting.models.base._base_model.BaseModel.transform_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform_output()</span></code></a></p></li>
<li><p><strong>use_metric</strong> (<em>bool</em>) – if to use metric to convert for conversion, if False,
simply take the average over <code class="docutils literal notranslate"><span class="pre">out[&quot;prediction&quot;]</span></code></p></li>
<li><p><strong>**kwargs</strong> – arguments to metric <code class="docutils literal notranslate"><span class="pre">to_quantiles</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>predictions of shape batch_size x timesteps</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.to_quantiles">
<span class="sig-name descname"><span class="pre">to_quantiles</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.to_quantiles"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_quantiles" title="Link to this definition">#</a></dt>
<dd><p>Convert output to quantiles using the loss metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – output of network where “prediction” has been
transformed with <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output" title="pytorch_forecasting.models.base._base_model.BaseModel.transform_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform_output()</span></code></a></p></li>
<li><p><strong>use_metric</strong> (<em>bool</em>) – if to use metric to convert for conversion, if False,
simply take the quantiles over <code class="docutils literal notranslate"><span class="pre">out[&quot;prediction&quot;]</span></code></p></li>
<li><p><strong>**kwargs</strong> – arguments to metric <code class="docutils literal notranslate"><span class="pre">to_quantiles</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>quantiles of shape batch_size x timesteps x n_quantiles</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.training_step" title="Link to this definition">#</a></dt>
<dd><p>Train on batch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.transform_output">
<span class="sig-name descname"><span class="pre">transform_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.Metric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric"><span class="pre">Metric</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.transform_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output" title="Link to this definition">#</a></dt>
<dd><p>Extract prediction from network output and rescale it to real space / de-normalize it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prediction</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>List</em><em>[</em><em>torch.Tensor</em><em>]</em><em>]</em>) – normalized prediction</p></li>
<li><p><strong>target_scale</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>List</em><em>[</em><em>torch.Tensor</em><em>]</em><em>]</em>) – scale to rescale prediction</p></li>
<li><p><strong>loss</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric.html#pytorch_forecasting.metrics.base_metrics._base_metrics.Metric" title="pytorch_forecasting.metrics.base_metrics._base_metrics.Metric"><em>Metric</em></a><em>]</em>) – metric to use for transform</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>rescaled prediction</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_forecasting/models/base/_base_model.html#BaseModel.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.validation_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the validation set. In this step you’d might generate examples or
calculate anything of interest like accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The output of your data iterable, normally a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><strong>batch_idx</strong> – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> – The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.validation_step" title="pytorch_forecasting.models.base._base_model.BaseModel.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;val_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;val_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#pytorch_forecasting.models.base._base_model.BaseModel.validation_step" title="pytorch_forecasting.models.base._base_model.BaseModel.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.current_stage">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_stage</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.current_stage" title="Link to this definition">#</a></dt>
<dd><p>Available inside lightning loops.
:return: current trainer stage. One of [“train”, “val”, “test”, “predict”, “sanity_check”]</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.log_interval">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_interval</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_interval" title="Link to this definition">#</a></dt>
<dd><p>Log interval depending if training or validating</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.n_targets">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_targets</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.n_targets" title="Link to this definition">#</a></dt>
<dd><p>Number of targets to forecast.</p>
<p>Based on loss function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>number of targets</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.pkg">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">pkg</span></span><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.pkg" title="Link to this definition">#</a></dt>
<dd><p>Package class for the model.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pytorch_forecasting.models.base._base_model.BaseModel.target_names">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">target_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#pytorch_forecasting.models.base._base_model.BaseModel.target_names" title="Link to this definition">#</a></dt>
<dd><p>List of targets that are predicted.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>list of target names</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pytorch_forecasting.models.base._base_model.AutoRegressiveBaseModelWithCovariates.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">AutoRegressiveBaseModelWithCovariates</p>
      </div>
    </a>
    <a class="right-next"
       href="pytorch_forecasting.models.base._base_model.BaseModelWithCovariates.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BaseModelWithCovariates</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel"><code class="docutils literal notranslate"><span class="pre">BaseModel</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">BaseModel.configure_optimizers()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.create_log"><code class="docutils literal notranslate"><span class="pre">BaseModel.create_log()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.deduce_default_output_parameters"><code class="docutils literal notranslate"><span class="pre">BaseModel.deduce_default_output_parameters()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.forward"><code class="docutils literal notranslate"><span class="pre">BaseModel.forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.from_dataset"><code class="docutils literal notranslate"><span class="pre">BaseModel.from_dataset()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.log"><code class="docutils literal notranslate"><span class="pre">BaseModel.log()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_gradient_flow"><code class="docutils literal notranslate"><span class="pre">BaseModel.log_gradient_flow()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_metrics"><code class="docutils literal notranslate"><span class="pre">BaseModel.log_metrics()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_prediction"><code class="docutils literal notranslate"><span class="pre">BaseModel.log_prediction()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_after_backward"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_after_backward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_epoch_end"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_epoch_end()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_load_checkpoint()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_save_checkpoint()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_test_epoch_end"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_test_epoch_end()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_train_epoch_end()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">BaseModel.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.plot_prediction"><code class="docutils literal notranslate"><span class="pre">BaseModel.plot_prediction()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict"><code class="docutils literal notranslate"><span class="pre">BaseModel.predict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_dependency"><code class="docutils literal notranslate"><span class="pre">BaseModel.predict_dependency()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.predict_step"><code class="docutils literal notranslate"><span class="pre">BaseModel.predict_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.size"><code class="docutils literal notranslate"><span class="pre">BaseModel.size()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.step"><code class="docutils literal notranslate"><span class="pre">BaseModel.step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.test_step"><code class="docutils literal notranslate"><span class="pre">BaseModel.test_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_prediction"><code class="docutils literal notranslate"><span class="pre">BaseModel.to_prediction()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.to_quantiles"><code class="docutils literal notranslate"><span class="pre">BaseModel.to_quantiles()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.training_step"><code class="docutils literal notranslate"><span class="pre">BaseModel.training_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.transform_output"><code class="docutils literal notranslate"><span class="pre">BaseModel.transform_output()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.validation_step"><code class="docutils literal notranslate"><span class="pre">BaseModel.validation_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.current_stage"><code class="docutils literal notranslate"><span class="pre">BaseModel.current_stage</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.log_interval"><code class="docutils literal notranslate"><span class="pre">BaseModel.log_interval</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.n_targets"><code class="docutils literal notranslate"><span class="pre">BaseModel.n_targets</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.pkg"><code class="docutils literal notranslate"><span class="pre">BaseModel.pkg</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch_forecasting.models.base._base_model.BaseModel.target_names"><code class="docutils literal notranslate"><span class="pre">BaseModel.target_names</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/api/pytorch_forecasting.models.base._base_model.BaseModel.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2020, Jan Beitner.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>