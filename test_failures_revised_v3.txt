============================= test session starts ==============================
platform darwin -- Python 3.14.3, pytest-9.0.2, pluggy-1.6.0 -- /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/bin/python
cachedir: .cache
rootdir: /Users/admin/Desktop/open-source/pytorch-forecasting
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 13 items / 12 deselected / 1 selected
run-last-failure: rerun previous 1 failure

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] FAILED [100%]

=================================== FAILURES ===================================
_ test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-33/test_decoder_mlp_supported_los0')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.NegativeBinomialDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        def int_target_collate(batch):
            # Handle both lists and tuples, filter out None
            filtered_batch = [item for item in batch if item is not None]
            if not filtered_batch:
                return {}, ()
    
            from torch.utils.data._utils.collate import default_collate
    
            # Collate x
            x_batch = [item[0] for item in filtered_batch]
            x_collated = default_collate(x_batch)
    
            # Collate y (target, weight)
            y_batch = [item[1] for item in filtered_batch]
            target_batch = [item[0] for item in y_batch]
            target_collated = default_collate(target_batch)
    
            weight_batch = [item[1] for item in y_batch]
            if any(w is None for w in weight_batch):
                weight_collated = None
            else:
                weight_collated = default_collate(weight_batch)
    
            # target should be long if we reach here (only used for NegativeBinomial)
            x_collated["encoder_lengths"] = x_collated["encoder_length"]
            x_collated["decoder_lengths"] = x_collated["decoder_length"]
            return x_collated, (target_collated.long(), weight_collated)
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1121: in _run_stage
    self._run_sanity_check()
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1150: in _run_sanity_check
    val_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/utilities.py:179: in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:146: in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:441: in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:329: in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:412: in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:741: in validation_step
    log, out = self.step(x, y, batch_idx)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:942: in step
    out = self(x, **kwargs)
          ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1776: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1787: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DecoderMLP(
  	"activation_class":                  ReLU
  	"categorical_groups":                {}
  	"dataset_parame...Norm((8,), eps=1e-05, elementwise_affine=True)
      (16): Linear(in_features=8, out_features=2, bias=True)
    )
  )
)
x = {'decoder_length': tensor([3, 3]), 'decoder_lengths': tensor([3, 3]), 'encoder_length': tensor([4, 4]), 'encoder_lengths': tensor([4, 4]), ...}
n_samples = None

    def forward(
        self, x: dict[str, torch.Tensor], n_samples: int = None
    ) -> dict[str, torch.Tensor]:
        """
        Forward network
        """
        # x is a batch generated based on the TimeSeriesDataset
        batch_size = x["decoder_lengths"].size(0)
        embeddings = self.input_embeddings(
>           x["decoder_cat"]
        )  # returns dictionary with embedding tensors
E       KeyError: 'decoder_cat'

pytorch_forecasting/models/mlp/_decodermlp.py:174: KeyError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                             | Params | Mode  | FLOPs
--------------------------------------------------------------------------------------
0 | loss             | NegativeBinomialDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                       | 0      | train | 0    
2 | input_embeddings | MultiEmbedding                   | 0      | train | 0    
3 | mlp              | FullyConnectedModule             | 314    | train | 0    
--------------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
=============================== warnings summary ===============================
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1006: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.
  See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.
    object_columns = data.head(1).select_dtypes(object).columns

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1885: Pandas4Warning: The copy keyword is deprecated and will be removed in a future version. Copy-on-Write is active in pandas since 3.0 which utilizes a lazy copy mechanism that defers copies until necessary. Use .copy() to make an eager copy if necessary.
    df_index = df_index[minimal_columns].astype("int32", copy=False)

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================= 1 failed, 12 deselected, 5 warnings in 0.40s =================
