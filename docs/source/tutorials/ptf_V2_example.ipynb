{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzVbXsEBxnF-"
      },
      "source": [
        "# `pytorch-forecasting v2` Model Training and Inference - Beta API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt0uZV7Px-40"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        ":warning: The vignette showcased here is part of an experimental rework of the `pytorch-forecasting` data layer, planned for release in v2.0.0. The API is currently unstable and subject to change without prior notice.\n",
        "\n",
        "Feedback and suggestions are highly encouraged â€” please share them in <a href=\"https://github.com/sktime/pytorch-forecasting/issues/1736\">issue 1736</a>.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D9ARyp05R0t"
      },
      "source": [
        "In this vignette, we demonstrate how to train and evaluate the **Temporal Fusion Transformer (TFT)** using the new `TimeSeries` and `DataModule` API from the v2 pipeline.\n",
        "\n",
        "\n",
        "## Steps\n",
        "\n",
        "1. **Load Data**  \n",
        "2. **Create Dataset & DataModule**  \n",
        "3. **Initialize, Train & Run Inference with the Model**\n",
        "\n",
        "\n",
        "\n",
        "### Load Data\n",
        "\n",
        "We generate a synthetic dataset using `load_toydata` which returns a `pandas` DataFrame with purely numerical values.  \n",
        "*(Note: The current pipeline assumes all inputs are numerical only.)*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Create Dataset & DataModule\n",
        "\n",
        "- `TimeSeries` returns the raw data in terms of tensors .\n",
        "- `DataModule` wraps the dataset, handles splits, preprocessing, batching, and exposes `metadata` for the model initialisation.\n",
        "\n",
        "\n",
        "\n",
        "### Initialize the Model\n",
        "\n",
        "We initialize the TFT model using the `metadata` provided by the `DataModule`. This metadata includes all required dimensional info for the encoder, decoder, and static inputs.\n",
        "\n",
        "\n",
        "\n",
        "### Train the Model\n",
        "\n",
        "We use a `Trainer` from PyTorch Lightning to train the model\n",
        "\n",
        "### Run Inference\n",
        "\n",
        "After training, we can make predictions using the trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMFNk4MyY_b"
      },
      "source": [
        "# 1. Load Data\n",
        "We generate a synthetic dataset using `load_toydata` that creates a `pandas` DataFrame with just numerical values as for now **the pipeline assumes the data to be numerical only**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RkgOT4kiy_RU"
      },
      "outputs": [],
      "source": [
        "from pytorch_forecasting.data.examples import load_toydata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WX-FRdusJSVN",
        "outputId": "e481484c-b0c3-4026-c933-a9dc047617c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "series_id",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "time_idx",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "x",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "y",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "category",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "future_known_feature",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "static_feature",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "static_feature_cat",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "6a4618cb-2cbb-4cfa-a518-68e6a491e237",
              "rows": [
                [
                  "0",
                  "0",
                  "0",
                  "-0.06544648294250992",
                  "0.06656137698496997",
                  "0",
                  "1.0",
                  "0.7967866945075578",
                  "0"
                ],
                [
                  "1",
                  "0",
                  "1",
                  "0.06656137698496997",
                  "0.41153330960960627",
                  "0",
                  "0.9950041652780258",
                  "0.7967866945075578",
                  "0"
                ],
                [
                  "2",
                  "0",
                  "2",
                  "0.41153330960960627",
                  "0.5111692057859301",
                  "0",
                  "0.9800665778412416",
                  "0.7967866945075578",
                  "0"
                ],
                [
                  "3",
                  "0",
                  "3",
                  "0.5111692057859301",
                  "0.5490171876676307",
                  "0",
                  "0.955336489125606",
                  "0.7967866945075578",
                  "0"
                ],
                [
                  "4",
                  "0",
                  "4",
                  "0.5490171876676307",
                  "0.8122499209297881",
                  "0",
                  "0.9210609940028851",
                  "0.7967866945075578",
                  "0"
                ]
              ],
              "shape": {
                "columns": 8,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>series_id</th>\n",
              "      <th>time_idx</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>category</th>\n",
              "      <th>future_known_feature</th>\n",
              "      <th>static_feature</th>\n",
              "      <th>static_feature_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.065446</td>\n",
              "      <td>0.066561</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.796787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.066561</td>\n",
              "      <td>0.411533</td>\n",
              "      <td>0</td>\n",
              "      <td>0.995004</td>\n",
              "      <td>0.796787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.411533</td>\n",
              "      <td>0.511169</td>\n",
              "      <td>0</td>\n",
              "      <td>0.980067</td>\n",
              "      <td>0.796787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.511169</td>\n",
              "      <td>0.549017</td>\n",
              "      <td>0</td>\n",
              "      <td>0.955336</td>\n",
              "      <td>0.796787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.549017</td>\n",
              "      <td>0.812250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.921061</td>\n",
              "      <td>0.796787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   series_id  time_idx         x         y  category  future_known_feature  \\\n",
              "0          0         0 -0.065446  0.066561         0              1.000000   \n",
              "1          0         1  0.066561  0.411533         0              0.995004   \n",
              "2          0         2  0.411533  0.511169         0              0.980067   \n",
              "3          0         3  0.511169  0.549017         0              0.955336   \n",
              "4          0         4  0.549017  0.812250         0              0.921061   \n",
              "\n",
              "   static_feature  static_feature_cat  \n",
              "0        0.796787                   0  \n",
              "1        0.796787                   0  \n",
              "2        0.796787                   0  \n",
              "3        0.796787                   0  \n",
              "4        0.796787                   0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_series = 100  # Number of individual time series to generate\n",
        "seq_length = 50  # Length of each time series\n",
        "data_df = load_toydata(num_series, seq_length)\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYQ5CdNUyc2q"
      },
      "source": [
        "# 2. Create the dataset and datamodule\n",
        "We create a `TimeSeries` dataset instance that returns the raw data in terms of tensors, then this \"raw data\" is sent to the `data_module`that will internally handle the dataloaders and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONe8Eo1zzvCH"
      },
      "source": [
        "`TimeSeries` dataset's Key arguments:\n",
        "- `data`: DataFrame with sequence data.\n",
        "- `time`: integer typed column denoting the time index within `data`.\n",
        "- `target`:  Column(s) in `data` denoting the forecasting target.\n",
        "- `group`: List of column names identifying a time series instance within `data`.\n",
        "- `num`: List of numerical features.\n",
        "- `cat`: List of categorical features.\n",
        "- `known`: Features known in future\n",
        "- `unknown`: Features not known in the future\n",
        "- `static`: List of variables that do not change over time,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JPD3y3qny5Dx"
      },
      "outputs": [],
      "source": [
        "from pytorch_forecasting.data.timeseries import TimeSeries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AxxPHK6AKSD2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\data\\timeseries\\_timeseries_v2.py:105: UserWarning: TimeSeries is part of an experimental rework of the pytorch-forecasting data layer, scheduled for release with v2.0.0. The API is not stable and may change without prior warning. For beta testing, but not for stable production use. Feedback and suggestions are very welcome in pytorch-forecasting issue 1736, https://github.com/sktime/pytorch-forecasting/issues/1736\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# create `TimeSeries` dataset that returns the raw data in terms of tensors\n",
        "dataset = TimeSeries(\n",
        "    data=data_df,\n",
        "    time=\"time_idx\",\n",
        "    target=\"y\",\n",
        "    group=[\"series_id\"],\n",
        "    num=[\"x\", \"future_known_feature\", \"static_feature\"],\n",
        "    cat=[\"category\", \"static_feature_cat\"],\n",
        "    known=[\"future_known_feature\"],\n",
        "    unknown=[\"x\", \"category\"],\n",
        "    static=[\"static_feature\", \"static_feature_cat\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-yHU46v1MhN"
      },
      "source": [
        "`EncoderDecoderTimeSeriesDataModule` key arguments:\n",
        "- `time_series_dataset`: `TimeSeries` dataset instance\n",
        "- `max_encoder_length` : Maximum length of the encoder input sequence.\n",
        "- `max_prediction_length` : Maximum length of the decoder output sequence.\n",
        "- `batch_size` : Batch size for DataLoader.\n",
        "- `categorical_encoders` :  Dictionary of categorical encoders.\n",
        "- `scalers` : Dictionary of feature scalers.\n",
        "- `target_normalizer`: Normalizer for the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DUWB4LrGyxrL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from pytorch_forecasting.data.data_module import EncoderDecoderTimeSeriesDataModule\n",
        "from pytorch_forecasting.data.encoders import (\n",
        "    NaNLabelEncoder,\n",
        "    TorchNormalizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U5Lr_ZFKX0s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\data\\data_module.py:129: UserWarning: EncoderDecoderTimeSeriesDataModule is part of an experimental rework of the pytorch-forecasting data layer, scheduled for release with v2.0.0. The API is not stable and may change without prior warning. For beta testing, but not for stable production use. Feedback and suggestions are very welcome in pytorch-forecasting issue 1736, https://github.com/sktime/pytorch-forecasting/issues/1736\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# create the `data_module` that handles the dataloaders and preprocessing\n",
        "data_module = EncoderDecoderTimeSeriesDataModule(\n",
        "    time_series_dataset=dataset,\n",
        "    max_encoder_length=30,\n",
        "    max_prediction_length=1,\n",
        "    batch_size=32,\n",
        "    categorical_encoders={\n",
        "        \"category\": NaNLabelEncoder(add_nan=True),\n",
        "        \"static_feature_cat\": NaNLabelEncoder(add_nan=True),\n",
        "    },\n",
        "    scalers={\n",
        "        \"x\": TorchNormalizer(),\n",
        "        \"future_known_feature\": TorchNormalizer(),\n",
        "        \"static_feature\": TorchNormalizer(),\n",
        "    },\n",
        "    target_normalizer=TorchNormalizer(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qykX7vQ7zWnC"
      },
      "source": [
        "# 3. Initialise and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kz3MO362Tlo"
      },
      "source": [
        "To initialise the model you now don't have to pass arguments like `encoder_cont`, `decoder_cont` etc as they are calculated internally using the `metadata` property [[source]](https://github.com/sktime/pytorch-forecasting/blob/4a34931e499c2b59de3939fcffcaabd75204b045/pytorch_forecasting/data/data_module.py#L264-L292) of `EncoderDecoderTimeSeriesDataModule`. But you still have to pass other params like `loss`, `optimizer` etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvwIuzD34Ytk"
      },
      "source": [
        "\n",
        "```python\n",
        "model = TFT(\n",
        "    loss=nn.MSELoss(),\n",
        "    logging_metrics=[MAE(), SMAPE()],\n",
        "    metadata=data_module.metadata,  # <-- crucial for model setup\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "The `metadata` includes:\n",
        "- `max_encoder_length`, `max_prediction_length`\n",
        "- number of continuous/categorical variables in encoder/decoder\n",
        "- number of static features\n",
        "\n",
        "These are used to configure internal layers like `encoder_cont`, `decoder_cat`, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xOsEucZnzCkN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pytorch_forecasting.metrics import MAE, SMAPE\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer._tft_v2 import TFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qbjnTxnyh4H",
        "outputId": "f59bf985-ffaa-4980-c890-39a80dfcc598"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\models\\base\\_base_model_v2.py:61: UserWarning: The Model 'TFT' is part of an experimental reworkof the pytorch-forecasting model layer, scheduled for release with v2.0.0. The API is not stable and may change without prior warning. This class is intended for beta testing and as a basic skeleton, but not for stable production use. Feedback and suggestions are very welcome in pytorch-forecasting issue 1736, https://github.com/sktime/pytorch-forecasting/issues/1736\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialise the Model\n",
        "model = TFT(\n",
        "    loss=MAE(),\n",
        "    logging_metrics=[MAE(), SMAPE()],\n",
        "    optimizer=\"adam\",\n",
        "    optimizer_params={\"lr\": 1e-3},\n",
        "    lr_scheduler=\"reduce_lr_on_plateau\",\n",
        "    lr_scheduler_params={\"mode\": \"min\", \"factor\": 0.1, \"patience\": 10},\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    attention_head_size=4,\n",
        "    dropout=0.1,\n",
        "    metadata=data_module.metadata,  # pass the metadata from the datamodule to the model\n",
        "    # to initialise important params like `encoder_cont` etc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svdoye-d8F-z"
      },
      "source": [
        "We use a `Trainer` from PyTorch Lightning to train the model:\n",
        "\n",
        "```python\n",
        "trainer = Trainer(max_epochs=5, ...)\n",
        "trainer.fit(model, data_module)\n",
        "```\n",
        "\n",
        "The `Trainer`:\n",
        "- Pulls data from `data_module`\n",
        "- Handles device placement\n",
        "- Logs training progress and metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RTSmUu9RytS8"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930,
          "referenced_widgets": [
            "edc196b34a0b49fa992fd89909a8414f",
            "76e801c32da348788b0b92fd2c2849c1",
            "cd6d925c2dcf4ae995f62d8f20219fde",
            "283f67f9ef6a47f8b3d4a389a18b8ccc",
            "8e19d683c674403b87cbb7c162a0ab33",
            "525fe2cef0444d558d870c394cf67e81",
            "640318d6256a4451957fce6481625f6b",
            "0820bd0fd60f4e0180899f18615d9ee3",
            "90cb2b96e18948679eda809a9123565c",
            "5ffb5772ae4f4764b23643cfa547a615",
            "04f140582780402a94e3391bfcbffa91",
            "4abfcb0ade1b47e1b015e7efd75cd563",
            "483c0919c7c740af89e6c7a7470591f7",
            "18849509bcfc41dd89011efb5690e49b",
            "098eb27809314e7ca15c5b9bce21b46d",
            "53f7fc6124fb4dda8bfb147078212ada",
            "8c2821dc533c4a1ba81d68836047dc54",
            "cb303a9d13084d47b0714d717f482a80",
            "491aa926179b41dba21a008dc6e9a4fc",
            "363967a782db4627a3e16a726b4993c6",
            "0d28163142aa465784871ec647efa504",
            "a6d821b9d9c9453b87740c3e4d86ca77",
            "2301684175b0454897af0fc5e534f52f",
            "58fe6653d5e944be8fdd072cd3541b8c",
            "00fb395e4cb5472faf221affc0244780",
            "1cd0c86474a443b8aae6466405dd3807",
            "cfcb344ba8cb49a78a8f934657252855",
            "4f455332437a465bb300ccf50405358d",
            "fde9c4616cd84e629998819c18adfd1f",
            "405e6f4ab49b445ba7e942e0d35419eb",
            "2ee2fe3ea3c04592851fef7e6dda5115",
            "713bcea77e1744179502f6a187239ce1",
            "531c73851dcd469791d6ebdc9aa9da52",
            "b0ed53cd20bd47b5aac5fecaacb9bfed",
            "da87d95793f84907819ab4c506f6936b",
            "c8a64619bf574d43a01595e3c8690759",
            "8d0c66b5cc984b8db7d151a970897266",
            "a723c1055f4f4c0d9c7b2c8c3469fa9d",
            "99dcc8dd7e88463a95ddcf86c67ec153",
            "94c1a818e78a47ee9bb46f1c8b1dad34",
            "903b0891a3014701b117a391d777e012",
            "f6053af158304fc1b307a296cdbe7290",
            "f6c93805c03545458f1af0fffaf77fb0",
            "52bbe8a40baf4a8f989281bcc225a067",
            "c9eb21c70ac243d1a21f50f287b4731f",
            "100a61bf42b0498ba241ddd9ef7a96f3",
            "1cce85eee5bc4700aa966c05e35ea042",
            "2dcaeafc8bc84db786f9fa99aeda39db",
            "7a9cf1446cc14d51a0a729822eb1cabc",
            "1c6f0e1be6f8475485ba6354ccc68597",
            "7ef67c160b8740e9932486f857470542",
            "282e2d3a169a4e18b46ca85dbfe82371",
            "84a96c68feba48b6880de08722de8743",
            "c111af2e297641908cb1835d9a4d06ff",
            "d94d97f0852b450c8ea56e6ef4e44ac5",
            "1dd910df34e44cdc98586e3094dfddd4",
            "86c6678f86d944eeba1fe982197cb7d3",
            "b609e7acf93944a98f11b450e01222f8",
            "0baf540b09454f61873cd6a3088a3c9d",
            "7bc8c547554c481481d0653193dcd917",
            "2d92b66364844b5491dabee7a0b90686",
            "99a8fec0a22f4949a3d16fc7f48aeea6",
            "0fb99264aa0644cda962905dfe1d6997",
            "f3d688e29f4b4432ac370d6a1e76ec5c",
            "d4cd0f3144784991ba9ec0b178a6c2e7",
            "83a1e407cf60401b932c6ee6e72d90db",
            "99443f56f20a4c6da81ddb107ab84490",
            "2da428c9a3b541bc8f667d5715fdcb0d",
            "f483a26167424f66b8b3c537537afd77",
            "bc01c61ee2a14f3bbea23d04bb7e9dc9",
            "b59e64ed8d9d46dd8803bfe25602c391",
            "cc1d7b4924784a08999f41e1e3c989ac",
            "8c6ad939d53942e48d8d6fb4c30b666d",
            "49d44a5db92b4b3e979e88d14072a16c",
            "36d0bfd0973c43e4958af452d2c7025c",
            "8a90ddfc1fd446a6894c2771195a4419",
            "aa94917a2f364a4ea9033000730d48b7"
          ]
        },
        "id": "aB_ayE_eykXp",
        "outputId": "02c49d3e-2124-4b0b-8ca4-b2d886662613"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3050 6GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\data\\encoders.py:626: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
            "  self.scale_ = torch.std(y_scale, dim=-1) + eps\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name                  | Type               | Params | Mode \n",
            "---------------------------------------------------------------------\n",
            "0 | loss                  | MAE                | 0      | train\n",
            "1 | encoder_var_selection | Sequential         | 709    | train\n",
            "2 | decoder_var_selection | Sequential         | 193    | train\n",
            "3 | static_context_linear | Linear             | 192    | train\n",
            "4 | lstm_encoder          | LSTM               | 51.5 K | train\n",
            "5 | lstm_decoder          | LSTM               | 50.4 K | train\n",
            "6 | self_attention        | MultiheadAttention | 16.6 K | train\n",
            "7 | pre_output            | Linear             | 4.2 K  | train\n",
            "8 | output_layer          | Linear             | 65     | train\n",
            "---------------------------------------------------------------------\n",
            "123 K     Trainable params\n",
            "0         Non-trainable params\n",
            "123 K     Total params\n",
            "0.495     Total estimated model params size (MB)\n",
            "18        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a052a210a3b48718a81abbd3c229f5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\metrics\\base_metrics\\_base_metrics.py:905: UserWarning: Loss is not finite. Resetting it to 1e9\n",
            "  warnings.warn(\"Loss is not finite. Resetting it to 1e9\")\n",
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\metrics\\base_metrics\\_base_metrics.py:905: UserWarning: Loss is not finite. Resetting it to 1e9\n",
            "  warnings.warn(\"Loss is not finite. Resetting it to 1e9\")\n",
            "c:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8c4b1870b4f401fb6eb23bf4bd671a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\pytorch_forecasting\\metrics\\base_metrics\\_base_metrics.py:905: UserWarning: Loss is not finite. Resetting it to 1e9\n",
            "  warnings.warn(\"Loss is not finite. Resetting it to 1e9\")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m trainer = Trainer(\n\u001b[32m      4\u001b[39m     max_epochs=\u001b[32m5\u001b[39m,\n\u001b[32m      5\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     log_every_n_steps=\u001b[32m10\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:202\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    205\u001b[39m     params_with_grad: List[Tensor] = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._zero_grad_fn()\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m    216\u001b[39m \u001b[38;5;28mself\u001b[39m.post_backward(closure_loss)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     **kwargs: Any,\n\u001b[32m     61\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     62\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28mself\u001b[39m._fabric.backward(loss, *args, **kwargs)\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\code\\pytorch-forecasting\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"\\nTraining model...\")\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    enable_progress_bar=True,\n",
        "    log_every_n_steps=10,\n",
        ")\n",
        "\n",
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3mI-QVJ8TZF"
      },
      "source": [
        "After training, we can make predictions using the trained model:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch = next(iter(data_module.test_dataloader()))\n",
        "    x, y = batch\n",
        "    y_pred = model(x)\n",
        "```\n",
        "\n",
        "#### Output\n",
        "Output of TFT model is a `dict` with key `prediction`:\n",
        "\n",
        "- `y_pred[\"prediction\"]`: Tensor of shape `(batch_size, prediction_length, output_size)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "a081e60ec1604f6abf87dc3f108d66b6",
            "565d40dbd4b0467396949cc2adc030d5",
            "f41ef269f3754fffa1296278be09be3b",
            "2f336cafec17479f80f897042286233a",
            "ccd55c64fc884543a5831f23cf213642",
            "e1ade849ec7341e9a026a7d2dcac24be",
            "105949ce9b8844a78481da2f7b7406b2",
            "ebc84674fde647aeb1bf579a3c890f15",
            "f82e1c40a89c4d14b28d91ed7d8a9e28",
            "a8fc76e0722a4dea8f00f5cd412399c6",
            "a84df45d2ed644f1baf72bd22f924224"
          ]
        },
        "id": "Si7bbZIULBZz",
        "outputId": "ff3fb499-14e2-48e5-e4a1-ec8e18650c36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/aryan/pytorch-forecasting/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating model...\n",
            "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 27.38it/s] \n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "       Test metric             DataLoader 0\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "        test_MAE            0.11830110847949982\n",
            "       test_SMAPE           0.4569336473941803\n",
            "        test_loss           0.11830110847949982\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Prediction shape: torch.Size([32, 1, 1])\n",
            "First prediction values: [[0.03997546]]\n",
            "First true values: [-0.16256696]\n",
            "\n",
            "TFT model test complete!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_metrics = trainer.test(model, data_module)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_batch = next(iter(data_module.test_dataloader()))\n",
        "    x_test, y_test = test_batch\n",
        "    y_pred = model(x_test)\n",
        "\n",
        "    print(\"\\nPrediction shape:\", y_pred[\"prediction\"].shape)\n",
        "    print(\"First prediction values:\", y_pred[\"prediction\"][0].cpu().numpy())\n",
        "    print(\"First true values:\", y_test[0].cpu().numpy())\n",
        "print(\"\\nTFT model test complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
