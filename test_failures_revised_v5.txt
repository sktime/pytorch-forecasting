============================= test session starts ==============================
platform darwin -- Python 3.14.3, pytest-9.0.2, pluggy-1.6.0 -- /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/bin/python
cachedir: .cache
rootdir: /Users/admin/Desktop/open-source/pytorch-forecasting
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 13 items / 12 deselected / 1 selected
run-last-failure: rerun previous 1 failure

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] FAILED [100%]

=================================== FAILURES ===================================
_ test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-37/test_decoder_mlp_supported_los0')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.NegativeBinomialDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            original_collate = train_dataloader.collate_fn
    
            def int_target_collate(batch):
                x, y = original_collate(batch)
                target, weight = y
                if isinstance(target, torch.Tensor):
                    y = (target.long(), weight)
                elif isinstance(target, list | tuple):
                    y = ([t.long() for t in target], weight)
                return x, y
    
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1121: in _run_stage
    self._run_sanity_check()
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1150: in _run_sanity_check
    val_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/utilities.py:179: in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:139: in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
                                       ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/fetchers.py:134: in __next__
    batch = super().__next__()
            ^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/fetchers.py:61: in __next__
    batch = next(self.iterator)
            ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/combined_loader.py:341: in __next__
    out = next(self._iterator)
          ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/combined_loader.py:142: in __next__
    out = next(self.iterators[0])
          ^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:741: in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:801: in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/utils/data/_utils/fetch.py:57: in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

batch = [({'decoder_length': np.int64(3), 'encoder_length': np.int64(4), 'encoder_target': tensor([106.2720, 133.8120,  70.308...080, 110.5920, 122.0400]), 'encoder_time_idx_start': tensor(35), ...}, (tensor([115.8840, 152.3880, 128.1960]), None))]

    def int_target_collate(batch):
        x, y = original_collate(batch)
        target, weight = y
>       if isinstance(target, torch.Tensor):
                              ^^^^^
E       NameError: name 'torch' is not defined

tests/test_models/test_mlp.py:219: NameError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                             | Params | Mode  | FLOPs
--------------------------------------------------------------------------------------
0 | loss             | NegativeBinomialDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                       | 0      | train | 0    
2 | input_embeddings | MultiEmbedding                   | 0      | train | 0    
3 | mlp              | FullyConnectedModule             | 314    | train | 0    
--------------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
=============================== warnings summary ===============================
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1006: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.
  See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.
    object_columns = data.head(1).select_dtypes(object).columns

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1885: Pandas4Warning: The copy keyword is deprecated and will be removed in a future version. Copy-on-Write is active in pandas since 3.0 which utilizes a lazy copy mechanism that defers copies until necessary. Use .copy() to make an eager copy if necessary.
    df_index = df_index[minimal_columns].astype("int32", copy=False)

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
  /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================= 1 failed, 12 deselected, 5 warnings in 0.39s =================
