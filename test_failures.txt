============================= test session starts ==============================
platform darwin -- Python 3.14.3, pytest-9.0.2, pluggy-1.6.0 -- /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/bin/python
cachedir: .cache
rootdir: /Users/admin/Desktop/open-source/pytorch-forecasting
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 2 items
run-last-failure: rerun previous 2 failures (skipped 29 files)

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] FAILED [ 50%]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3] FAILED [100%]                                                                   

=================================== FAILURES ===================================
_ test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-27/test_decoder_mlp_supported_los0')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.NegativeBinomialDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='identity',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        def int_target_collate(batch):
            # Handle both lists and tuples, filter out None
            try:
                filtered_batch = [item for item in list(batch) if item is not None]
            except Exception:
                filtered_batch = [] if batch is None else [batch]
            if not filtered_batch:
                # Return an empty dict to avoid collate errors
                return {}
            from torch.utils.data._utils.collate import default_collate
            try:
                collated = default_collate(filtered_batch)
            except Exception:
                # Fallback: just return the filtered batch if collate fails
                return filtered_batch
            if isinstance(collated, dict) and "target" in collated:
                collated["target"] = collated["target"].long()
            return collated
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1121: in _run_stage
    self._run_sanity_check()
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1150: in _run_sanity_check
    val_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/utilities.py:179: in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:146: in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:441: in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:329: in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:412: in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:741: in validation_step
    log, out = self.step(x, y, batch_idx)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DecoderMLP(
  	"activation_class":                  ReLU
  	"categorical_groups":                {}
  	"dataset_parame...Norm((8,), eps=1e-05, elementwise_affine=True)
      (16): Linear(in_features=8, out_features=2, bias=True)
    )
  )
)
x = ({'decoder_length': np.int64(3), 'encoder_length': np.int64(4), 'encoder_target': tensor([106.2720, 133.8120,  70.3080, 110.5920]), 'encoder_time_idx_start': tensor(34), ...}, (tensor([122.0400, 115.8840, 152.3880]), None))
y = ({'decoder_length': np.int64(3), 'encoder_length': np.int64(4), 'encoder_target': tensor([133.8120,  70.3080, 110.5920, 122.0400]), 'encoder_time_idx_start': tensor(35), ...}, (tensor([115.8840, 152.3880, 128.1960]), None))
batch_idx = 0, kwargs = {}

    def step(
        self,
        x: dict[str, torch.Tensor],
        y: tuple[torch.Tensor, torch.Tensor],
        batch_idx: int,
        **kwargs,
    ) -> tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:
        """
        Run for each train/val step.
    
        Args:
            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader
            y (Tuple[torch.Tensor, torch.Tensor]): y as passed to the loss function by the dataloader
            batch_idx (int): batch number
            **kwargs: additional arguments to pass to the network apart from ``x``
    
        Returns:
            Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]: tuple where the first
                entry is a dictionary to which additional logging results can be added for consumption in the
                ``on_epoch_end`` hook and the second entry is the model's output.
        """  # noqa: E501
        # pack y sequence if different encoder lengths exist
>       if (x["decoder_lengths"] < x["decoder_lengths"].max()).any():
            ^^^^^^^^^^^^^^^^^^^^
E       TypeError: tuple indices must be integers or slices, not str

pytorch_forecasting/models/base/_base_model.py:857: TypeError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                             | Params | Mode  | FLOPs
--------------------------------------------------------------------------------------
0 | loss             | NegativeBinomialDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                       | 0      | train | 0    
2 | input_embeddings | MultiEmbedding                   | 0      | train | 0    
3 | mlp              | FullyConnectedModule             | 314    | train | 0    
--------------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
_ test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3] _

data_with_covariates =          agency     sku    volume  ...     weight  time_idx  target
0     Agency_22  SKU_01   52.2720  ...   8.229938 ...1.963758        59   1.000
6772  Agency_22  SKU_04   72.0153  ...   9.486183        59   1.000

[180 rows x 31 columns]
tmp_path = PosixPath('/private/var/folders/99/j2sfgnsj2kzf8t0bg8850j5w0000gn/T/pytest-of-admin/pytest-27/test_decoder_mlp_supported_los1')
loss_cls = <class 'pytorch_forecasting.metrics.distributions.LogNormalDistributionLoss'>
target = 'volume'
data_loader_kwargs = {'target': 'volume', 'target_normalizer': GroupNormalizer(
	method='standard',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation='log',
	method_kwargs={}
)}
normalizer = GroupNormalizer(
	method='standard',
	groups=None,
	center=False,
	scale_by_group=False,
	transformation='log',
	method_kwargs={}
)

    @pytest.mark.parametrize(
        "loss_cls, target, data_loader_kwargs, normalizer",
        [
            (NormalDistributionLoss, "volume", {}, None),
            (NegativeBinomialDistributionLoss, "volume", {}, GroupNormalizer(method="identity", center=False)),
            (MultivariateNormalDistributionLoss, "volume", {}, None),
            (LogNormalDistributionLoss, "volume", {}, GroupNormalizer(center=False, transformation="log")),
            (BetaDistributionLoss, "volume", {}, GroupNormalizer(center=True, transformation="logit")),
            (ImplicitQuantileNetworkDistributionLoss, "volume", {}, None),
        ],
    )
    def test_decoder_mlp_supported_losses(
        data_with_covariates,
        tmp_path,
        loss_cls,
        target,
        data_loader_kwargs,
        normalizer,
    ):
    
        data_loader_kwargs = dict(data_loader_kwargs)
        data_loader_kwargs["target"] = target
        if normalizer is not None:
            data_loader_kwargs["target_normalizer"] = normalizer
    
        # Always copy data to avoid accidental overwrite
        data = data_with_covariates.copy()
    
        def int_target_collate(batch):
            # Handle both lists and tuples, filter out None
            try:
                filtered_batch = [item for item in list(batch) if item is not None]
            except Exception:
                filtered_batch = [] if batch is None else [batch]
            if not filtered_batch:
                # Return an empty dict to avoid collate errors
                return {}
            from torch.utils.data._utils.collate import default_collate
            try:
                collated = default_collate(filtered_batch)
            except Exception:
                # Fallback: just return the filtered batch if collate fails
                return filtered_batch
            if isinstance(collated, dict) and "target" in collated:
                collated["target"] = collated["target"].long()
            return collated
    
        # Preprocess target for distribution requirements
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            # Targets must be strictly non-negative integers (>=0)
            data["target"] = data["target"].apply(lambda x: max(0, int(round(x))))
            # After normalization, cast to int again to ensure integer type
            def to_int_if_possible(x):
                try:
                    return int(round(x))
                except Exception:
                    return 0
            data["target"] = data["target"].apply(to_int_if_possible)
        elif loss_cls.__name__ == "LogNormalDistributionLoss":
            # Targets must be strictly positive (>0)
            data["target"] = data["target"].apply(lambda x: max(1e-3, float(x)))
            # Ensure no zero or negative values after normalization
            data["target"] = data["target"].apply(lambda x: x if x > 0 else 1e-3)
        elif loss_cls.__name__ == "BetaDistributionLoss":
            # Targets must be strictly between 0 and 1 (not including endpoints)
            data["target"] = data["target"].apply(lambda x: min(max(x, 1e-3), 1-1e-3))
    
        dataloaders = make_dataloaders(
            data, **data_loader_kwargs
        )
        train_dataloader = dataloaders["train"]
        val_dataloader = dataloaders["val"]
    
        if loss_cls.__name__ == "NegativeBinomialDistributionLoss":
            train_dataloader.collate_fn = int_target_collate
            val_dataloader.collate_fn = int_target_collate
        net = DecoderMLP.from_dataset(
            train_dataloader.dataset,
            loss=loss_cls(),
            learning_rate=0.01,
            hidden_size=8,
        )
        trainer = pl.Trainer(
            max_epochs=1,
            enable_checkpointing=False,
            logger=False,
            limit_train_batches=1,
            limit_val_batches=1,
        )
    
    
>       trainer.fit(
            net,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )

tests/test_models/test_mlp.py:250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:584: in fit
    call._call_and_handle_interrupt(
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:49: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:630: in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1079: in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1121: in _run_stage
    self._run_sanity_check()
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/trainer.py:1150: in _run_sanity_check
    val_loop.run()
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/utilities.py:179: in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:146: in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
.venv/lib/python3.14/site-packages/lightning/pytorch/loops/evaluation_loop.py:441: in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/trainer/call.py:329: in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/lightning/pytorch/strategies/strategy.py:412: in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:741: in validation_step
    log, out = self.step(x, y, batch_idx)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/models/base/_base_model.py:954: in step
    loss = self.loss(prediction, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1776: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1787: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torchmetrics/metric.py:315: in forward
    self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torchmetrics/metric.py:384: in _forward_reduce_state_update
    self.update(*args, **kwargs)
.venv/lib/python3.14/site-packages/torchmetrics/metric.py:549: in wrapped_func
    update(*args, **kwargs)
pytorch_forecasting/metrics/base_metrics/_base_metrics.py:887: in update
    losses = self.loss(y_pred, target)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/metrics/base_metrics/_base_metrics.py:1046: in loss
    distribution = self.map_x_to_distribution(y_pred)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pytorch_forecasting/metrics/distributions.py:237: in map_x_to_distribution
    return self.distribution_class(loc=x[..., 0], scale=x[..., 1])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/distributions/log_normal.py:47: in __init__
    base_dist = Normal(loc, scale, validate_args=validate_args)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.14/site-packages/torch/distributions/normal.py:67: in __init__
    super().__init__(batch_shape, validate_args=validate_args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Normal(loc: torch.Size([2, 3]), scale: torch.Size([2, 3]))
batch_shape = torch.Size([2, 3]), event_shape = torch.Size([])
validate_args = None

    def __init__(
        self,
        batch_shape: torch.Size = torch.Size(),
        event_shape: torch.Size = torch.Size(),
        validate_args: Optional[bool] = None,
    ) -> None:
        self._batch_shape = batch_shape
        self._event_shape = event_shape
        if validate_args is not None:
            self._validate_args = validate_args
        if self._validate_args:
            try:
                arg_constraints = self.arg_constraints
            except NotImplementedError:
                arg_constraints = {}
                warnings.warn(
                    f"{self.__class__} does not define `arg_constraints`. "
                    + "Please set `arg_constraints = {}` or initialize the distribution "
                    + "with `validate_args=False` to turn off validation.",
                    stacklevel=2,
                )
            for param, constraint in arg_constraints.items():
                if constraints.is_dependent(constraint):
                    continue  # skip constraints that cannot be checked
                if param not in self.__dict__ and isinstance(
                    getattr(type(self), param), lazy_property
                ):
                    continue  # skip checking lazily-constructed args
                value = getattr(self, param)
                valid = constraint.check(value)
                if not torch._is_all_true(valid):
>                   raise ValueError(
                        f"Expected parameter {param} "
                        f"({type(value).__name__} of shape {tuple(value.shape)}) "
                        f"of distribution {repr(self)} "
                        f"to satisfy the constraint {repr(constraint)}, "
                        f"but found invalid values:\n{value}"
                    )
E                   ValueError: Expected parameter scale (Tensor of shape (2, 3)) of distribution Normal(loc: torch.Size([2, 3]), scale: torch.Size([2, 3])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:
E                   tensor([[-5.6608e+14, -9.9343e+14, -9.6124e+14],
E                           [-5.6608e+14, -9.9343e+14, -9.6124e+14]])

.venv/lib/python3.14/site-packages/torch/distributions/distribution.py:78: ValueError
----------------------------- Captured stdout call -----------------------------
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
----------------------------- Captured stderr call -----------------------------
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.

  | Name             | Type                      | Params | Mode  | FLOPs
-------------------------------------------------------------------------------
0 | loss             | LogNormalDistributionLoss | 0      | train | 0    
1 | logging_metrics  | ModuleList                | 0      | train | 0    
2 | input_embeddings | MultiEmbedding            | 0      | train | 0    
3 | mlp              | FullyConnectedModule      | 314    | train | 0    
-------------------------------------------------------------------------------
314       Trainable params
0         Non-trainable params
314       Total params
0.001     Total estimated model params size (MB)
28        Modules in train mode
0         Modules in eval mode
0         Total Flops
=============================== warnings summary ===============================
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1006: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.
  See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.
    object_columns = data.head(1).select_dtypes(object).columns

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/pytorch_forecasting/data/timeseries/_timeseries.py:1885: Pandas4Warning: The copy keyword is deprecated and will be removed in a future version. Copy-on-Write is active in pandas since 3.0 which utilizes a lazy copy mechanism that defers copies until necessary. Use .copy() to make an eager copy if necessary.
    df_index = df_index[minimal_columns].astype("int32", copy=False)

tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[NegativeBinomialDistributionLoss-volume-data_loader_kwargs1-normalizer1]
tests/test_models/test_mlp.py::test_decoder_mlp_supported_losses[LogNormalDistributionLoss-volume-data_loader_kwargs3-normalizer3]
  /Users/admin/Desktop/open-source/pytorch-forecasting/.venv/lib/python3.14/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================== 2 failed, 10 warnings in 0.58s ========================
